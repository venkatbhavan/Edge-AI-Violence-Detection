# ğŸ›¡ï¸ Edge-AI Based Real-Time Violence Detection using ESP32-CAM

**Author:** Venkat Bhavan Tati  
**Matrikel-Nr:** 1575011  
**Course:** Industrial AI â€“ Edge AI in Industrial Applications (AI5252)  
**Semester:** WiSe 25/26  

---

## ğŸ“Œ Overview

This project implements a distributed **Edge-AI system** for real-time violence detection using:

- ğŸ“· ESP32-CAM (Edge Sensing Device)
- ğŸ§  MobileNetV2-based CNN (Edge Inference Model)
- ğŸŒ Embedded Web Interface (Flask Server)
- ğŸ“± Multi-device remote access (Mobile / Tablet)

The system performs real-time classification of violent vs non-violent actions using motion-based preprocessing and lightweight deep learning inference.

The architecture demonstrates a **multi-edge deployment strategy** suitable for industrial safety monitoring environments.

---

# ğŸ—ï¸ System Architecture

The system consists of two edge devices and remote client devices:

### ğŸ”¹ Edge Device 1 â€“ ESP32-CAM
- Captures live video
- Streams frames via Wi-Fi
- Performs no AI inference

### ğŸ”¹ Edge Device 2 â€“ Edge Inference Node (Laptop)
- Motion preprocessing
- CNN inference (MobileNetV2)
- Threshold-based decision logic
- Embedded web service hosting

### ğŸ”¹ Remote Client Devices
- Smartphone / Tablet
- Access live results via IP-based web interface
- No inference performed on client

---

## ğŸ”„ System Workflow

ESP32-CAM â†’ Wi-Fi â†’ Edge Inference Node â†’ CNN Classification â†’ Web Interface â†’ Mobile Client


1. ESP32-CAM captures frames  
2. Frames transmitted via HTTP  
3. Motion preprocessing applied  
4. CNN performs inference  
5. Violence probability computed  
6. Results visualized locally  
7. Embedded server streams output  
8. Mobile device accesses via browser  

---

# ğŸ§  AI Methodology

## Binary Classification

- Class 0 â†’ Non-Violence  
- Class 1 â†’ Violence  

---

## Motion-Based Representation

To emphasize dynamic behavior:

Motion(t) = |Frame(t) âˆ’ Frame(tâˆ’1)|


This:
- Suppresses static background
- Reduces lighting sensitivity
- Highlights aggressive movement patterns

---

## Model Architecture

- Base Model: **MobileNetV2**
- Transfer Learning from ImageNet
- Input: 96 Ã— 96 grayscale
- Output: Sigmoid activation
- Lightweight architecture suitable for edge deployment

---

# ğŸ“Š Model Performance

Total samples: **55,085**

| Metric | Value |
|--------|-------|
| Accuracy | 96% |
| Precision (Violence) | 0.95 |
| Recall (Violence) | 0.97 |
| Weighted F1-Score | 0.9565 |

Low false negative rate is especially important for safety-critical monitoring systems.

---

# ğŸŒ Embedded Edge Deployment

The trained model is deployed using a lightweight Flask server.

### Run the Web Server

```bash
python stepG_web_embedded_violence.py
Then open in your browser:

http://<your-ip-address>:5000
Accessible from any device on the same local network.

This ensures:

No cloud dependency

Local inference only

Multi-device accessibility

ğŸ” GDPR & Privacy Considerations
The system follows privacy-by-design principles:

No persistent storage of video data

Frames processed in volatile memory only

No cloud transmission

No facial recognition

No biometric identification

Operates within local network

The system performs scene-level classification only and is intended as an assistive monitoring tool.

ğŸ“ Repository Structure
esp32_cam_project/
â”‚
â”œâ”€â”€ model/
â”‚   â””â”€â”€ action_violence_model.h5
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ step3_capture_frames.py
â”‚   â”œâ”€â”€ step4_motion_preprocessing.py
â”‚   â”œâ”€â”€ step5_collect_action_dataset.py
â”‚   â”œâ”€â”€ stepC_train_action_model.py
â”‚   â”œâ”€â”€ stepD_live_violence_detection.py
â”‚   â”œâ”€â”€ stepF_esp32_cam_violence_detection.py
â”‚   â”œâ”€â”€ stepG_web_embedded_violence.py
â”‚   â””â”€â”€ stepI_app_phone_camera_violence.py
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â””â”€â”€ classification_report.txt
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
ğŸ“ Dataset
The dataset is not included in this repository due to size and licensing constraints.

To reproduce training:

Download a public violence dataset.

Convert videos to frames.

Apply motion preprocessing.

Place processed data in:

processed_dataset/
    â”œâ”€â”€ violence/
    â””â”€â”€ non_violence/
âš™ï¸ Installation
1ï¸âƒ£ Create Environment
conda create -n violence_ai python=3.9
conda activate violence_ai
2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt
â–¶ï¸ Running the System
Laptop Camera
python stepD_live_violence_detection.py
ESP32-CAM
python stepF_esp32_cam_violence_detection.py
Embedded Web Server
python stepG_web_embedded_violence.py
ğŸ¯ Conclusion
This project demonstrates a distributed Edge-AI system combining:

Edge sensing (ESP32-CAM)

Lightweight CNN inference

Real-time classification

Embedded web deployment

Multi-device accessibility

GDPR-conscious design

The architecture reflects practical industrial Edge-AI deployment strategies and provides a scalable foundation for further enhancements.